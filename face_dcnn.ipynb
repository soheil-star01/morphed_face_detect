{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "face_dcnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuhe3Si9s_u"
      },
      "source": [
        "# !pip install opencv-contrib-python==4.4.0.44"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuFDRCgfZJH6",
        "outputId": "302494a2-8853-4478-8d2e-9eac0e87550e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr9EHbM457Cv"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPeNcLvpswhq"
      },
      "source": [
        "### defining some parameters.\n",
        "\n",
        "class config:\n",
        "    CDNN_SHAPE = (400, 400) ### the shape of input images for DCNN \n",
        "    TRAINING_SET_SIZE = 1100 ### number of training samples\n",
        "    TESTING_SET_SIZE = 600 ### number of testing samples\n",
        "    VALIDATION_SET_SIZE = 200 ### number of validation samples \n",
        "    N_CLUSTER = 50 ### NUMBER OF VISUAL WORDS  *OR*  NUMBER OF FEATURES  *OR*  number of clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSoFOCzY6lw9",
        "outputId": "7ba082c0-01a3-4aa3-e5b1-4507a650d393"
      },
      "source": [
        "path_dataset = '/content/drive/MyDrive/projects/estee/NEW_DATASET'\n",
        "\n",
        "b_images = os.listdir(path_dataset+'/BONAFIDEA') ### get all of `bonafide` files name\n",
        "m_images = os.listdir(path_dataset+'/morphed') ### get all of `morphed` files name\n",
        "\n",
        "print(f'number of Bonafide images: {len(b_images)}\\nnumber of Morphed images: {len(m_images)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of Bonafide images: 920\n",
            "number of Morphed images: 1066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9uqRgax9h30"
      },
      "source": [
        "### creating a Pandas Dataframe with file name and class type (morphe/bonafide)\n",
        "\n",
        "df_all = pd.DataFrame(b_images,columns=['file'])\n",
        "df_all['class'] = 0\n",
        "df_all_ = pd.DataFrame(m_images,columns=['file'])\n",
        "df_all_['class'] = 1\n",
        "df_all = pd.concat((df_all, df_all_),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ4UnlJ8GaaF"
      },
      "source": [
        "### train test split. I used stratify to keep distribution of the whole dataset\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_all['file'].to_numpy(), \n",
        "                                                    df_all['class'].to_numpy(), \n",
        "                                                    test_size=0.33, \n",
        "                                                    random_state=1370,\n",
        "                                                    stratify=df_all['class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaEeA1WUx-SH"
      },
      "source": [
        "## DCNN_ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXefPASMEPKD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet101V2, InceptionV3, VGG19, EfficientNetB7\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
        "from sklearn.utils import shuffle\n",
        "from keras.utils.np_utils import to_categorical \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SgsYE3-VQdD"
      },
      "source": [
        "### Data augmentation\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "\n",
        "\n",
        "def aug_rotate(img, angle, bg_patch=(5,5)):\n",
        "    rgb = len(img.shape) == 3\n",
        "    bg_color = np.mean(img[:bg_patch[0], :bg_patch[1], :], axis=(0,1))\n",
        "    img = rotate(img, angle, reshape=False)\n",
        "    mask = [img <= 0, np.any(img <= 0, axis=-1)][rgb]\n",
        "    img[mask] = bg_color\n",
        "    return img\n",
        "\n",
        "def aug_gaussian_noise(img, mean=0, sigma=0.03):\n",
        "    img = img.copy()\n",
        "    noise = np.random.normal(mean, sigma, img.shape)*255\n",
        "    mask_overflow_upper = img+noise >= 255.0\n",
        "    mask_overflow_lower = img+noise < 0\n",
        "    noise[mask_overflow_upper] = 255.0\n",
        "    noise[mask_overflow_lower] = 0\n",
        "    return img + noise.astype(int)\n",
        "\n",
        "\n",
        "def aug_random_crop(img, crop_size=config.CDNN_SHAPE):\n",
        "    img = img.copy()\n",
        "    w, h = img.shape[:2]\n",
        "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
        "    img = img[y:y+crop_size[0], x:x+crop_size[1]]\n",
        "    return img\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMVXtKD0Kkc7"
      },
      "source": [
        "### to avoid loading all images at once, I used Batch generator technique.\n",
        "### in each step of each epoch, it only load `batch_size_` number of images\n",
        "\n",
        "\n",
        "def create_generator(batch_size_, X_input, y_input, training=False):\n",
        "    X_train_ = X_input\n",
        "    y_train_ = y_input\n",
        "    while 1:\n",
        "        X_train_, y_train_ = shuffle(X_train_, y_train_)\n",
        "        for j in range(batch_size_):\n",
        "            type_f = '/BONAFIDEA/' if y_train_[j]==0 else '/morphed/'\n",
        "            img_ = cv2.imread(path_dataset+type_f+X_train_[j])\n",
        "            img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            if training:\n",
        "                if np.random.uniform()>0.8:\n",
        "                    img_ = aug_gaussian_noise(img_, mean=(1-2*np.random.rand())*0.01, sigma=(np.random.rand())*0.05)\n",
        "                img_ = aug_rotate(img_, np.random.randint(-20,20), bg_patch=(5,5))\n",
        "                img_ = aug_random_crop(img_)\n",
        "            else:\n",
        "                row_ = int(img_.shape[0]/2)\n",
        "                col_ = int(img_.shape[1]/2)\n",
        "                s1   = int(min(img_.shape[:2])/2)-2 \n",
        "                img_ = img_[row_-s1:row_+s1, :][: ,col_-s1:col_+s1]\n",
        "                img_ = cv2.resize(img_, config.CDNN_SHAPE)\n",
        "\n",
        "            img_ = img_.reshape((1,)+img_.shape)\n",
        "            lbl_ = y_train_[j]\n",
        "            if j == 0:\n",
        "                x = img_\n",
        "                y = [lbl_]\n",
        "            else:\n",
        "                x = np.concatenate((x,img_))\n",
        "                y.append(lbl_)\n",
        "        if len(X_train_) > 2*batch_size_:\n",
        "            X_train_ = X_train_[batch_size_:]\n",
        "            y_train_ = y_train_[batch_size_:]\n",
        "        else:\n",
        "            X_train_ = X_input\n",
        "            y_train_ = y_input\n",
        "        yield (x/255).astype(np.float16), to_categorical(y, num_classes=2)    #changed from 16 to 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawmFftWgIi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0eb6d4-dc84-43c8-d216-edb18970ab3b"
      },
      "source": [
        "def create_model(base_model):\n",
        "    base_model.trainable = True\n",
        "    global_average_layer = GlobalAveragePooling2D()(base_model.output)\n",
        "    prediction_layer = Dense(2, activation='softmax')(global_average_layer)\n",
        "    model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
        "    # model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def fit_model(model):\n",
        "    TRAIN_GEN_SIZE = min(20,config.TRAINING_SET_SIZE) ### number of images to be loaded at each step at each epoch, for training\n",
        "    VAL_GEN_SIZE = min(30,config.VALIDATION_SET_SIZE) ### number of images to be loaded at each step at each epoch, for testing\n",
        "\n",
        "\n",
        "\n",
        "    get_train_set_ = create_generator(TRAIN_GEN_SIZE, \n",
        "                                    X_train[:config.TRAINING_SET_SIZE], \n",
        "                                    y_train[:config.TRAINING_SET_SIZE],\n",
        "                                    training=False)\n",
        "\n",
        "    get_val_set_ = create_generator(VAL_GEN_SIZE, \n",
        "                                    X_train[config.TRAINING_SET_SIZE:config.VALIDATION_SET_SIZE+config.TRAINING_SET_SIZE], \n",
        "                                    y_train[config.TRAINING_SET_SIZE:config.VALIDATION_SET_SIZE+config.TRAINING_SET_SIZE])\n",
        "    \n",
        "    earlystopper = EarlyStopping(patience=2, verbose=1) ### to prevent over fitting I used earlystop + validation set\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(  monitor='val_accuracy', ### to reduce learining rate when the optimization doesnt go furture\n",
        "                                    factor=0.5,\n",
        "                                    patience=2, \n",
        "                                    min_lr=0.000001, \n",
        "                                    verbose=1,  \n",
        "                                    cooldown=1)\n",
        "    callback_list = [reduce_lr,earlystopper] ### I added early stop and reduce learining rate here\n",
        "\n",
        "    history = model.fit_generator(  get_train_set_, \n",
        "                                    validation_data=get_val_set_,\n",
        "                                    validation_steps=int(config.VALIDATION_SET_SIZE//VAL_GEN_SIZE),\n",
        "                                    epochs=10, \n",
        "                                    steps_per_epoch=int(config.TRAINING_SET_SIZE//TRAIN_GEN_SIZE),\n",
        "                                    callbacks=callback_list)\n",
        "    return history\n",
        "\n",
        "base_model1 = ResNet101V2(input_shape=(*config.CDNN_SHAPE, 3), include_top=False, weights=\"imagenet\")\n",
        "base_model2 = InceptionV3(input_shape=(*config.CDNN_SHAPE, 3), include_top=False, weights=\"imagenet\")\n",
        "base_model3 = VGG19(input_shape=(*config.CDNN_SHAPE, 3), include_top=False, weights=\"imagenet\")\n",
        "base_model4 = EfficientNetB7(input_shape=(*config.CDNN_SHAPE, 3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "\n",
        "model1 = create_model(base_model1)\n",
        "model2 = create_model(base_model2)\n",
        "model3 = create_model(base_model3)\n",
        "model4 = create_model(base_model4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171319296/171317808 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjb9AXFpho8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "04613454-1b6b-4bd3-aa37-61b8d3667262"
      },
      "source": [
        "history1 = fit_model(model1)\n",
        "model1.save(path_dataset+'/models/model1.h5')\n",
        "history2 = fit_model(model2)\n",
        "model2.save(path_dataset+'/models/model2.h5')\n",
        "history3 = fit_model(model3)\n",
        "model3.save(path_dataset+'/models/model3.h5')\n",
        "# history4 = fit_model(model4)\n",
        "# model4.save('models/model4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a4ce23954f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/models/model1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/models/model2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhistory3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fit_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "_LGjc_qfV-NM",
        "outputId": "a9adf0a6-2c76-4ffa-da32-6649a9b3caa1"
      },
      "source": [
        "def load_all_models():\n",
        "    all_models = []\n",
        "    model_names = ['model1.h5', 'model2.h5', 'model3.h5']\n",
        "    for model_name in model_names:\n",
        "        filename = os.path.join(path_dataset+'models', model_name)\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        all_models.append(model)\n",
        "        print('loaded:', filename)\n",
        "    return all_models\n",
        "\n",
        "models = load_all_models()\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "myinput = tf.keras.layers.Input(shape=a.input.shape)\n",
        "for model in models:\n",
        "    model = tf.keras.layers.Input(shape=a.input.shape)(model)\n",
        "# ensemble_visible = [model.input for model in models]\n",
        "ensemble_outputs = [model.output for model in models]\n",
        "merge = tf.keras.layers.concatenate(ensemble_outputs)\n",
        "merge = tf.keras.layers.Dense(10, activation='relu')(merge)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(merge)\n",
        "model = tf.keras.models.Model(inputs=myinput, outputs=output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded: /content/drive/MyDrive/projects/estee/NEW_DATASETmodels/model1.h5\n",
            "loaded: /content/drive/MyDrive/projects/estee/NEW_DATASETmodels/model2.h5\n",
            "loaded: /content/drive/MyDrive/projects/estee/NEW_DATASETmodels/model3.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7a00d9a2e106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensemble_visible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 204\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m       raise ValueError('The name \"' + name + '\" is used ' +\n\u001b[0;32m-> 1001\u001b[0;31m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' times in the model. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m                        'All layer names should be unique.')\n\u001b[1;32m   1003\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnetwork_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"concatenate\" is used 2 times in the model. All layer names should be unique."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcE_nE5Xn3-l"
      },
      "source": [
        "a = models[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLVl2vDZoEiH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8459QzXQn6KP",
        "outputId": "1707fe63-fec5-4cd4-ab6d-360352ee8fa1"
      },
      "source": [
        "a.input.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 400, 400, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnovvaSeppV"
      },
      "source": [
        "### to avoid loading all images at once, I used Batch generator technique.\n",
        "### in each step of each epoch, it only load `batch_size_` number of images\n",
        "\n",
        "\n",
        "def create_generator_(batch_size_, X_input, y_input, training=False):\n",
        "    X_train_ = X_input\n",
        "    y_train_ = y_input\n",
        "    while 1:\n",
        "        X_train_, y_train_ = shuffle(X_train_, y_train_)\n",
        "        for j in range(batch_size_):\n",
        "            type_f = '/BONAFIDEA/' if y_train_[j]==0 else '/morphed/'\n",
        "            img_ = cv2.imread(path_dataset+type_f+X_train_[j])\n",
        "            img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            if training:\n",
        "                if np.random.uniform()>0.8:\n",
        "                    img_ = aug_gaussian_noise(img_, mean=(1-2*np.random.rand())*0.01, sigma=(np.random.rand())*0.05)\n",
        "                img_ = aug_rotate(img_, np.random.randint(-20,20), bg_patch=(5,5))\n",
        "                img_ = aug_random_crop(img_)\n",
        "            else:\n",
        "                row_ = int(img_.shape[0]/2)\n",
        "                col_ = int(img_.shape[1]/2)\n",
        "                s1   = int(min(img_.shape[:2])/2)-2 \n",
        "                img_ = img_[row_-s1:row_+s1, :][: ,col_-s1:col_+s1]\n",
        "                img_ = cv2.resize(img_, config.CDNN_SHAPE)\n",
        "\n",
        "            img_ = img_.reshape((1,)+img_.shape)\n",
        "            lbl_ = y_train_[j]\n",
        "            if j == 0:\n",
        "                x = img_\n",
        "                y = [lbl_]\n",
        "            else:\n",
        "                x = np.concatenate((x,img_))\n",
        "                y.append(lbl_)\n",
        "        if len(X_train_) > 2*batch_size_:\n",
        "            X_train_ = X_train_[batch_size_:]\n",
        "            y_train_ = y_train_[batch_size_:]\n",
        "        else:\n",
        "            X_train_ = X_input\n",
        "            y_train_ = y_input\n",
        "        yield (x/255).astype(np.float16), to_categorical(y, num_classes=2)     #changed from 16 to 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-846zQZt7XEF",
        "outputId": "6c6256a8-974d-4580-fe73-46e10c83433a"
      },
      "source": [
        "len(model.input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9VbLLjqWCAp",
        "outputId": "caf39d45-68c7-42ce-82ff-f6a352b1fc5e"
      },
      "source": [
        "TRAIN_GEN_SIZE = min(20,config.TRAINING_SET_SIZE) ### number of images to be loaded at each step at each epoch, for training\n",
        "VAL_GEN_SIZE = min(30,config.VALIDATION_SET_SIZE) ### number of images to be loaded at each step at each epoch, for testing\n",
        "earlystopper = EarlyStopping(patience=2, verbose=1) ### to prevent over fitting I used earlystop + validation set\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(  monitor='val_accuracy', ### to reduce learining rate when the optimization doesnt go furture\n",
        "                                factor=0.5,\n",
        "                                patience=2, \n",
        "                                min_lr=0.000001, \n",
        "                                verbose=1,  \n",
        "                                cooldown=1)\n",
        "callback_list = [reduce_lr,earlystopper]\n",
        "\n",
        "get_val_set_ = create_generator_(VAL_GEN_SIZE, \n",
        "                                 X_train[config.TRAINING_SET_SIZE:config.VALIDATION_SET_SIZE+config.TRAINING_SET_SIZE], \n",
        "                                 y_train[config.TRAINING_SET_SIZE:config.VALIDATION_SET_SIZE+config.TRAINING_SET_SIZE])\n",
        "    \n",
        "get_train_set_ = create_generator_( TRAIN_GEN_SIZE, \n",
        "                                    X_train[:config.TRAINING_SET_SIZE], \n",
        "                                    y_train[:config.TRAINING_SET_SIZE],\n",
        "                                    training=False)\n",
        "\n",
        "history = model.fit_generator(  get_train_set_, \n",
        "                                validation_data=get_val_set_,\n",
        "                                validation_steps=int(config.VALIDATION_SET_SIZE//VAL_GEN_SIZE),\n",
        "                                epochs=10, \n",
        "                                steps_per_epoch=int(config.TRAINING_SET_SIZE//TRAIN_GEN_SIZE),\n",
        "                                callbacks=callback_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "55/55 [==============================] - 124s 2s/step - loss: 0.7024 - accuracy: 0.5350 - val_loss: 0.6368 - val_accuracy: 0.5556\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 109s 2s/step - loss: 0.6207 - accuracy: 0.8049 - val_loss: 0.5948 - val_accuracy: 0.9056\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 109s 2s/step - loss: 0.5698 - accuracy: 0.9500 - val_loss: 0.5515 - val_accuracy: 0.9111\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 109s 2s/step - loss: 0.5295 - accuracy: 0.9344 - val_loss: 0.5036 - val_accuracy: 0.9222\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 109s 2s/step - loss: 0.4885 - accuracy: 0.9312 - val_loss: 0.4758 - val_accuracy: 0.9000\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.4519 - accuracy: 0.9283 - val_loss: 0.4393 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.4080 - accuracy: 0.9385 - val_loss: 0.4137 - val_accuracy: 0.9167\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.3885 - accuracy: 0.9399 - val_loss: 0.4222 - val_accuracy: 0.8889\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.3732 - accuracy: 0.9400 - val_loss: 0.4060 - val_accuracy: 0.8944\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.3650 - accuracy: 0.9387 - val_loss: 0.3877 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5epD69geL2JB"
      },
      "source": [
        "model.save(path_dataset+'/models/model_ensemble.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "R3ysxCjDA27i",
        "outputId": "e290ed60-6902-4ad7-dcae-09e9831cf08c"
      },
      "source": [
        "### showing the confusion matrix for CDNN method\n",
        "\n",
        "TEST_GEN_SIZE = min(30,config.TESTING_SET_SIZE)\n",
        "\n",
        "get_test_set_ = create_generator_(TEST_GEN_SIZE, X_test[:config.TESTING_SET_SIZE], y_test[:config.TESTING_SET_SIZE])\n",
        "\n",
        "pred_cdnn_method = model.predict_generator(get_test_set_, steps=config.TESTING_SET_SIZE//TEST_GEN_SIZE)\n",
        "pred_cdnn_method_ = np.argmax(pred_cdnn_method, axis=1)\n",
        "\n",
        "conf_mat = confusion_matrix(y_test[:config.TESTING_SET_SIZE],\n",
        "                            pred_cdnn_method_)\n",
        "sns.heatmap(conf_mat, annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f44a5ea4ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIwicaWVwQvu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}